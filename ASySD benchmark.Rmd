---
title: "Benchmarking ASySD"
output: webexercises::webexercises_default
---

# Introduction

There are many competing deduplication tools - including R packages, functionalities of reference managers or modules in commercial software designed to support systematic reviews. McKeown and Mir [(2021)](https://doi.org/10.1186/s13643-021-01583-y) recently benchmarked six such tools, yet did not include R packages which could deliver greater transparency and integrate more seamlessly with our workflow. Therefore, we used their results and dataset to benchmark the [ASySD](https://github.com/camaradesuk/ASySD) R package.

# Benchmark
```{r, include=FALSE}
pacman::p_load(webexercises)
```

`r webexercises::hide("Click here to see the details of the benchmarking.")`

```{r setup, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
remotes::install_github("camaradesuk/ASySD", ref = remotes::github_pull("7"))
library(ASySD)
#pacman::p_load_current_gh("camaradesuk/ASySD")
pacman::p_load_gh("mjwestgate/synthesisr")
library(dplyr)
library(readr)
library(magrittr)
```

This is run with ASySD version `r `packageVersion("ASySD")`, the latest GitHub version on `r Sys.Date()`.

## Load data

```{r load-refs}
df <- read_refs(c("0_Ovid_search_results_1_to_1000.ris","0_Ovid_search_results_1001_to_2000.ris",
                  "0_Ovid_search_results_2001_to_3000.ris", "0_Ovid_search_results_3001_to_3130.ris"),
                  tag_naming = "ovid")
```

## Prep for ASySD and deduplicate


```{r dedup-refs}
df_prepped <- df %>% transmute(author, 
                 title, 
                 abstract, 
                 doi,
                 year,
                 journal,
                 volume,
                 number = issue,
                 pages = paste0(start_page, "-", end_page),
                 isbn = NA, 
                 label = NA,
                 source = NA) %>% tibble::rowid_to_column("record_id")


res <- dedup_citations(df_prepped)
```

## Compare to benchmark

```{r benchmark}
benchmark_groups <- read_csv("benchmark-groups.csv") %>% filter(!is.na(Dup_Group))

#Drop duplicated entries
deduplicated <- res$unique[!duplicated(res$unique$record_id),]

results <- benchmark_groups %>% 
  group_by(Dup_Group) %>%
  #Factor introduced to ensure that false positive and negative are always included - even when 0
  summarise(selected = factor(sum(ORN %in% deduplicated$record_id), levels = 0:20)) %>% 
  count(selected, .drop = FALSE) %>%
  mutate(selected = as.numeric(as.character(selected)),
         outcome = case_when(selected == 0 ~ "false positive",
                             selected == 1 ~ "correct",
                             selected > 1 ~ "false negative"),
         dummy = (selected - 1) * n) %>%
  mutate(
         N = case_when(outcome == "false negative" ~ as.integer(dummy),
                       outcome != "false negative" ~ as.integer(n))) %>%
  group_by(outcome) %>%
  summarise(N_rows = sum(N), N_records = sum(n))

results

results <- results$N_rows %>% set_names(results$outcome)

# Supposed false positive is an error in the benchmarking dataset
benchmark_groups %>% 
  group_by(Dup_Group) %>%
  #Factor introduced to ensure that false positive and negative are always included - even when 0
  summarise(selected = factor(sum(ORN %in% deduplicated$record_id), levels = 0:20)) %>% 
  filter(selected == 0) %>% 
  inner_join(benchmark_groups) %>% 
  select(Dup_Group, AU, TI, YR, JN)

# This is because one version of this article is wrongly in a different benchmarking group
benchmark_groups %>% 
  filter(Dup_Group %in% c("3", "4")) %>% 
  select(Dup_Group, AU, TI, YR, JN)

false_negatives <- benchmark_groups %>% 
  group_by(Dup_Group) %>%
  summarise(selected = sum(ORN %in% deduplicated$record_id), record_id = as.character(ORN[ORN %in% deduplicated$record_id])) %>% 
  filter(selected > 1) %>% 
  select(-selected) %>%
  inner_join(deduplicated)

false_negatives

results <- tibble::tribble(
  ~Measure,  ~Score,
  "accuracy", 1 - (results["false positive"] + results["false negative"])/nrow(df),
  "sensitivity", 1 - results["false negative"]/(nrow(benchmark_groups)-length(unique(benchmark_groups$Dup_Group))),
  "specificity", 1 - results["false positive"]/length(unique(benchmark_groups$Dup_Group))) %>% 
  gt::gt() %>% gt::fmt_percent(2, decimals = 1) %>% 
  gt::tab_header("ASySD performance on McKeown & Kir (2021) dataset")

```

`r webexercises::unhide()`

# Results

The benchmarking results are shown in the table below - though note that the only false positive is actually an error in the benchmarking dataset, to that the true specificity is 100%. 

```{r echo = FALSE}
results
```

With regard to sensitivity, ASySD's 91% outperforms most of the tools considered by McKeown and Kir (2021, see their [Table 3](https://link.springer.com/article/10.1186/s13643-021-01583-y/tables/3)). Only Rayyan offered significantly greater sensitivity, but at the cost of much lower specificity (97%, which implies that 3% of unique articles would be mistakenly 'lost' as duplicates). 

Also, the results here only consider ASySD's fully automated deduplication. Many of the false negatives are included in the list of possible further duplicates for manual review that is also provided. Therefore, we decided to rely on ASySD for deduplication in the meta-analysis of the diversity-team performance link.


